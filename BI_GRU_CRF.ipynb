{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Vectors....\n",
    "\n",
    "and define functions to convert words to vectors and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-processed Data\n",
    "\n",
    "(Minimally processed data - only used pre-trained GloVe and padding to a fixed length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('NERPICKLE_10000', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "NER_tags = processed_data[0]\n",
    "sentences = processed_data[1]\n",
    "tags = processed_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training, validating and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "\n",
    "train_len = int(.8*len(sentences))\n",
    "val_len = int(.1*len(sentences))\n",
    "test_len = len(sentences) - train_len - val_len\n",
    "\n",
    "train_sentences = sentences[0:train_len]\n",
    "train_tags = tags[0:train_len]\n",
    "\n",
    "val_sentences = sentences[train_len:train_len+val_len]\n",
    "val_tags = tags[train_len:train_len+val_len]\n",
    "\n",
    "test_sentences = sentences[train_len+val_len:]\n",
    "test_tags = tags[train_len+val_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create 'meta-batches' of sentences and the target NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(sentences,tags,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(sentences))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_sentence = []\n",
    "    batches_tag = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(sentences):\n",
    "        \n",
    "        batch_sentence = []\n",
    "        batch_tag = []\n",
    "\n",
    "        for j in xrange(i,i+batch_size):\n",
    "            \n",
    "            batch_sentence.append(sentences[shuffle[j]])\n",
    "            batch_tag.append(tags[shuffle[j]])\n",
    "            \n",
    "        batch_sentence = np.asarray(batch_sentence,np.float32)\n",
    "        batch_tag = np.asarray(batch_tag,np.int32)\n",
    "        \n",
    "        batches_sentence.append(batch_sentence)\n",
    "        batches_tag.append(batch_tag)\n",
    "\n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_sentence = np.asarray(batches_sentence,np.float32)\n",
    "    batches_tag = np.asarray(batches_tag,np.int32)\n",
    "\n",
    "    return batches_sentence,batches_tag\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "tf_sentences = tf.placeholder(tf.float32,[None,None,word_vec_dim])\n",
    "tf_tags = tf.placeholder(tf.int32,[None,None])\n",
    "traintestval = tf.placeholder(tf.bool)\n",
    "scale_down = 1\n",
    "\n",
    "hidden_size = 100\n",
    "learning_rate = 0.01\n",
    "beta = 0.0001\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "init = tf.zeros_initializer()\n",
    "    \n",
    "    \n",
    "with tf.variable_scope(\"Bi_GRU\"):\n",
    "\n",
    "    # FORWARD GRU PARAMETERS\n",
    "    \n",
    "    wzf = tf.get_variable(\"wzf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "    uzf = tf.get_variable(\"uzf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "    bzf = tf.get_variable(\"bzf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "    wrf = tf.get_variable(\"wrf\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "    urf = tf.get_variable(\"urf\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "    brf = tf.get_variable(\"brf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "    wf = tf.get_variable(\"wf\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "    uf = tf.get_variable(\"uf\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "    bf = tf.get_variable(\"bf\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "    # BACKWARD GRU PARAMETERS\n",
    "\n",
    "    wzb = tf.get_variable(\"wzb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "    uzb = tf.get_variable(\"uzb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "    bzb = tf.get_variable(\"bzb\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "    wrb = tf.get_variable(\"wrb\", shape=[word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "    urb = tf.get_variable(\"urb\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "    brb = tf.get_variable(\"brb\", shape=[hidden_size],initializer=init)\n",
    "\n",
    "    wb = tf.get_variable(\"wb\", shape=[word_vec_dim, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "    ub = tf.get_variable(\"ub\", shape=[hidden_size, hidden_size],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "    bb = tf.get_variable(\"bb\", shape=[hidden_size],initializer=init)\n",
    "    \n",
    "\n",
    "W_score = tf.get_variable(\"W_score\", shape=[2*hidden_size,len(NER_tags)],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                     regularizer=regularizer)\n",
    "\n",
    "B_score = tf.get_variable(\"B_score\", shape=[len(NER_tags)],initializer=init)\n",
    "\n",
    "Transition_matrix = tf.get_variable(\"T\",shape=[len(NER_tags),len(NER_tags)],\n",
    "                                    initializer=tf.random_normal_initializer(),\n",
    "                                    regularizer=regularizer)\n",
    "\n",
    "l1 = tf.get_variable(\"l1\", shape=[1],\n",
    "                     initializer=tf.constant_initializer(0.5),\n",
    "                     regularizer=regularizer)\n",
    "\n",
    "l2 = tf.get_variable(\"l2\", shape=[1],\n",
    "                     initializer=tf.constant_initializer(0.5),\n",
    "                     regularizer=regularizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for layer normalization\n",
    "\n",
    "Without scale and shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs,scope,epsilon = 1e-5):\n",
    "\n",
    "    mean, var = tf.nn.moments(inputs, [1,2], keep_dims=True)\n",
    "\n",
    "    LN = tf.multiply((1/ tf.sqrt(var + epsilon)),(inputs - mean))\n",
    " \n",
    "    return LN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones implementation of Bi-directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_GRU(inp,hidden,seq_len,scope):\n",
    "    \n",
    "    #inp shape = batch_size x seq_len x vector_dimension\n",
    "    \n",
    "    inp = tf.transpose(inp,[1,0,2])\n",
    "    \n",
    "    #now inp shape = seq_len x batch_size x vector_dimension\n",
    "    \n",
    "    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hiddenf = hidden\n",
    "    hiddenb = hidden\n",
    "\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        \n",
    "        wzf = tf.get_variable(\"wzf\")\n",
    "        uzf = tf.get_variable(\"uzf\")\n",
    "        bzf = tf.get_variable(\"bzf\")\n",
    "        \n",
    "        wrf = tf.get_variable(\"wrf\")\n",
    "        urf = tf.get_variable(\"urf\")\n",
    "        brf = tf.get_variable(\"brf\")\n",
    "        \n",
    "        wf = tf.get_variable(\"wf\")\n",
    "        uf = tf.get_variable(\"uf\")\n",
    "        bf = tf.get_variable(\"bf\")\n",
    "        \n",
    "        wzb = tf.get_variable(\"wzb\")\n",
    "        uzb = tf.get_variable(\"uzb\")\n",
    "        bzb = tf.get_variable(\"bzb\")\n",
    "        \n",
    "        wrb = tf.get_variable(\"wrb\")\n",
    "        urb = tf.get_variable(\"urb\")\n",
    "        brb = tf.get_variable(\"brb\")\n",
    "        \n",
    "        wb = tf.get_variable(\"wb\")\n",
    "        ub = tf.get_variable(\"ub\")\n",
    "        bb = tf.get_variable(\"bb\")\n",
    "        \n",
    "    i = 0\n",
    "    j = seq_len - 1\n",
    "    \n",
    "    def cond(i,j,hiddenf,hiddenb,hidden_forward,hidden_backward):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,j,hiddenf,hiddenb,hidden_forward,hidden_backward):\n",
    "        \n",
    "        xf = inp[i]\n",
    "        xb = inp[j]\n",
    "\n",
    "        # FORWARD GRU EQUATIONS:\n",
    "        z = tf.sigmoid( tf.matmul(xf,wzf) + tf.matmul(hiddenf,uzf) + bzf )\n",
    "        r = tf.sigmoid( tf.matmul(xf,wrf) + tf.matmul(hiddenf,urf) + brf )\n",
    "        h_ = tf.tanh( tf.matmul(xf,wf) + tf.multiply(r,tf.matmul(hiddenf,uf)) + bf )\n",
    "        hiddenf = tf.multiply(z,h_) + tf.multiply((1-z),hiddenf)\n",
    "\n",
    "        hidden_forward = hidden_forward.write(i,hiddenf)\n",
    "        \n",
    "        # BACKWARD GRU EQUATIONS:\n",
    "        z = tf.sigmoid( tf.matmul(xb,wzb) + tf.matmul(hiddenb,uzb) + bzb )\n",
    "        r = tf.sigmoid( tf.matmul(xb,wrb) + tf.matmul(hiddenb,urb) + brb )\n",
    "        h_ = tf.tanh( tf.matmul(xb,wb) + tf.multiply(r,tf.matmul(hiddenb,ub)) + bb )\n",
    "        hiddenb = tf.multiply(z,h_) + tf.multiply((1-z),hiddenb)\n",
    "        \n",
    "        hidden_backward = hidden_backward.write(j,hiddenb)\n",
    "        \n",
    "        \n",
    "        return i+1,j-1,hiddenf,hiddenb,hidden_forward,hidden_backward\n",
    "    \n",
    "    _,_,_,_,hidden_forward,hidden_backward = tf.while_loop(cond,body,[i,j,\n",
    "                                                                        hiddenf,\n",
    "                                                                        hiddenb,\n",
    "                                                                        hidden_forward,\n",
    "                                                                        hidden_backward])\n",
    "    \n",
    "    forward = hidden_forward.stack()\n",
    "    backward = hidden_backward.stack()\n",
    "    \n",
    "    hidden_list = tf.concat([forward,backward],2)\n",
    "    \n",
    "    #forward\\backward\\hidden_list shape = seq_len x  batch_size x 2*hidden_size\n",
    "    \n",
    "    hidden_list = tf.transpose(hidden_list,[1,0,2])\n",
    "    \n",
    "    #now hidden_list shape = batch_size x seq_len x 2*hidden_size\n",
    "    \n",
    "    return hidden_list\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "The fist part (The GRU layer) is straight forward. The input sentences are passed through a bi-directional GRU and then each words are transformed to a unnormalized probability distribution over the NER tags through a linear layer.\n",
    "\n",
    "The second part is a CRF layer. The following is the core equation of CRF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\n",
    "P(y_{1:N}\\mid x_{1:N})  =\\frac1Zexp( \\Sigma^N_{n=1} \\Sigma^F_{i=1} \\lambda_i f(y_{n-1},y_n,x_n)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here P(y1:n | x1: n) is the probability of the sequence y1,y2....yn given the inputs x1,x2,...xn.\n",
    "Z is the normalizing factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the CRF negative log likelihood of the P(y|x) can be used as the cost function, where y is the target sequence, and x is the input sequence. The equation of the cost function can be thus expressed as:-\n",
    "\n",
    "\\begin{align}\n",
    "-log(P(y_{1:N}\\mid x_{1:N})) &= -log(\\frac1Zexp( \\Sigma^N_{n=1} \\Sigma^F_{i=1} \\lambda_i f(y_{n-1},y_n,x_n))\\\\\n",
    "                             &= -(\\Sigma^N_{n=1} \\Sigma^F_{i=1} \\lambda_i f(y_{n-1},y_n,x_n) - log(Z))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z is the sum of \n",
    "\n",
    "\\begin{equation}\n",
    "exp( \\Sigma^N_{n=1} \\Sigma^F_{i=1} \\lambda_i f(y_{n-1},y_n,x_n)\n",
    "\\end{equation} \n",
    "\n",
    "for all possible sequence values: y1,y2,...yn. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\Sigma^S_{s=1}exp( \\Sigma^N_{n=1} \\Sigma^F_{i=1} \\lambda_i f(y_{s,n-1},y_{s,n},x_n)\n",
    "\\end{equation}\n",
    "\n",
    "Where s is the sequence no. ysn indicates the nth position of the sth sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma^F_{i=1} \\lambda_i f(y_{n-1},y_n,x_n)\n",
    "\\end{equation}\n",
    "\n",
    "I am using:\n",
    "\n",
    "\\begin{equation}\n",
    "l_1 GRUscores(x_n) + l_2 Transition_{y_{n-1},y_n}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1, and l2 are learnable parameters. Transition_yn-1,yn represents Transition_matrix[y_n-1][y_n] which constitutes a score for y_n (a given tag at position n) given that the previous tag (at n-1 position) is y_n-1.\n",
    "Transition_matrix is a matrix of learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the CRF layer, computes log(Z) and the second part computes the unormalized probability score of the target sequence (if we are testing, validation or training).\n",
    "The second part is a straight forward summation:\n",
    "\n",
    "\\begin{equation}\n",
    "score = \\Sigma^N_{n=1} l_1 GRUscores(x_n) + l_2 Transition_{y_{n-1},y_n}\n",
    "\\end{equation}\n",
    "\n",
    "This score is then used in the cost function as defined before. The cost function is basically:\n",
    "-(score - log(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part, that is, computing the Z of log(Z) is a bit troublesome. I used Dynamic programming is for this. To describe it briefly:\n",
    "    \n",
    "To calculate a the **sum of all sequences of size i,and ending with tag K** (S), I can use this:\n",
    "\n",
    "```\n",
    "for all j: \n",
    "e = exp(l1*GRU_score(position_i,tag_k) + Transition[j][K])\n",
    "Sk += e*sum_of_all_sequences_of_size_(i-1)_ending_with_j\n",
    "```   \n",
    "where sum_of_all_sequences_of_size_(i-1)_ending_with_j =\n",
    "```\n",
    "exp(unexponentiated score of i-1 sized seq 1 ending with j) + exp(unexponentiated score of i-1 sized seq 2 ending with j).....\n",
    "    \n",
    "```\n",
    "As a result of multiplying e with sum_of_all_sequences_of_size_(i-1)_ending_with_j we get:\n",
    "```\n",
    "exp(l1*GRU_score(position_i,tag_k)+ Transition[j][K])*exp(unexponentiated score of i-1 sized seq 1 ending with j) + exp(l1*GRU_score(position_i,tag_k)+ Transition[j][K])*exp(unexponentiated score of i-1 sized seq 2 ending with j).....\n",
    "```\n",
    "Which is:\n",
    "\n",
    "```\n",
    "exp(l1*GRU_score(position_i,tag_k)+ Transition[j][K] + unexp score of i-1 sized seq 1 ending with j) +\n",
    "exp(l1*GRU_score(position_i,tag_k)+ Transition[j][K] + unexp score of i-1 sized seq 2 ending with j).....\n",
    "```\n",
    "Which is:\n",
    "```\n",
    "exp(unexp score of i sized seq 1 ending with j) +\n",
    "exp(unexp score of i sized seq 2 ending with j).....\n",
    "```\n",
    "\n",
    "Now, we can do the same thing by adding logs:\n",
    "\n",
    "```\n",
    "for all j: \n",
    "unexp_e = l1*GRU_score(position_i,tag_k) + Transition[j][K]\n",
    "Sk += exp(unexp_e+log(sum_of_all_sequences_of_size_(i-1)_ending_with_j))\n",
    "```\n",
    "Because:\n",
    "```\n",
    "exp(unexp_e+log(sum_of_all_sequences_of_size_(i-1)_ending_with_j))  \n",
    "= exp(unexp_e)*exp(log(sum_of_all_sequences_of_size_(i-1)_ending_with_j))  \n",
    "= exp(unexp_e)*(sum_of_all_sequences_of_size_(i-1)_ending_with_j)  \n",
    "Which is equal to e*(sum_of_all_sequences_of_size_(i-1)_ending_with_j) \n",
    "```\n",
    "Doing this can bring more stability. For more stability I did something more:\n",
    "```\n",
    "let mj = unexp_e+log(sum_of_all_sequences_of_size_(i-1)_ending_with_j)\n",
    "So we get Sk = exp(m1) + exp(m2)....\n",
    "```\n",
    "\n",
    "exponentials of big no.s can can bring numerical instability, so I did this:\n",
    "\n",
    "```\n",
    "m_max = maximum(m1,m2....)\n",
    "```\n",
    "Sk = exp(m1-m_max) + exp(m2-m_max).....\n",
    "While taking the log(Sk) to add it to other components while computing Sk values for next sequence,\n",
    "I use:\n",
    "\n",
    "```\n",
    "m_max + log(Sk)\n",
    "\n",
    "```\n",
    "\n",
    "m_max + log(exp(m1-m_max) + exp(m2-m_max).....) is equivalent to log(exp(m1) + exp(m2).....) since:\n",
    "\n",
    "```\n",
    "m_max + log(exp(m1-m_max) + exp(m2-m_max).....)  \n",
    "= log(exp(m_max)) + log(exp(m1-m_max) + exp(m2-m_max).....)\n",
    "= log ( exp(m_max) * (exp(m1-m_max) + exp(m2-m_max).....))\n",
    "= log ( exp(m_max + m1 - m_max) + exp(m_max+m2-m_max)......)\n",
    "= log ( exp(m1) + exp(m2)....)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fn1(tf_batch_size,seq_len,y_score,scores):\n",
    "    \n",
    "    batch_indices = tf.range(tf_batch_size)\n",
    "        \n",
    "    batch_indices = tf.reshape(batch_indices,[-1,1])\n",
    "    tags_to_concat = tf.reshape(tf_tags[:,0],[-1,1])\n",
    "        \n",
    "    indices_batch_tag_i = tf.concat([batch_indices,tags_to_concat],1)\n",
    "        \n",
    "    y_score = (tf.gather_nd(scores[0],indices_batch_tag_i))\n",
    "    y_score = l1*y_score\n",
    "        \n",
    "    i=tf.constant(1)\n",
    "        \n",
    "    def cond(i,y_score):\n",
    "                \n",
    "        return i<seq_len\n",
    "            \n",
    "    def body(i,y_score):\n",
    "            \n",
    "        batch_indices = tf.range(tf_batch_size)\n",
    "        batch_indices = tf.reshape(batch_indices,[-1,1])\n",
    "        tags_to_concat = tf.reshape(tf_tags[:,i],[-1,1])\n",
    "            \n",
    "        indices_batch_tag_i = tf.concat([batch_indices,tags_to_concat],1)\n",
    "            \n",
    "        tags_i_1 = tf.reshape(tf_tags[:,i-1],[-1,1])\n",
    "        tags_i = tf.reshape(tf_tags[:,i],[-1,1])\n",
    "            \n",
    "        indices_i_1_to_i = tf.concat([tags_i_1,tags_i],1)\n",
    "        \n",
    "        GRU_scores = tf.gather_nd(scores[i],indices_batch_tag_i)\n",
    "        T_scores = tf.gather_nd(Transition_matrix,indices_i_1_to_i)\n",
    "        \n",
    "        y_score +=  l1*GRU_scores + l2*T_scores\n",
    "            \n",
    "        return i+1, y_score\n",
    "        \n",
    "    _,y_score = tf.while_loop(cond,body,[i,y_score])\n",
    "        \n",
    "    return y_score\n",
    "    \n",
    "def fn2(y_score):\n",
    "    return y_score\n",
    "\n",
    "\n",
    "def model(traintestval):\n",
    "\n",
    "    # GRU LAYER\n",
    "    \n",
    "    tf_batch_size = tf.shape(tf_sentences)[0]\n",
    "    seq_len = tf.shape(tf_sentences)[1]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32) \n",
    "    \n",
    "    hidden_list = bi_GRU(tf_sentences,hidden,seq_len,'Bi_GRU')\n",
    "    \n",
    "    hidden_list = tf.reshape(hidden_list,[-1,2*hidden_size])\n",
    "    \n",
    "    scores = tf.nn.relu(tf.matmul(hidden_list,W_score) + B_score)\n",
    "    \n",
    "    GRU_scores = tf.reshape(scores,[tf_batch_size,seq_len,len(NER_tags)])\n",
    "    \n",
    "    GRU_scores = layer_norm(GRU_scores,\"GRU_out\")\n",
    "    \n",
    "    # CRF LAYER\n",
    "    \n",
    "    scores =tf.transpose(GRU_scores,[1,0,2])\n",
    "\n",
    "    \n",
    "    # now scores shape = seq_len x batch_size x ner_tags\n",
    "    \n",
    "    scores = scores*scale_down\n",
    "    \n",
    "    i_1_scores = l1*scores[0]\n",
    "    \n",
    "    # now i_1_scores = batch_size x ner_tags\n",
    "    \n",
    "    i = tf.constant(1)\n",
    "    \n",
    "    def cond(i,i_1_scores):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,i_1_scores):\n",
    "        \n",
    "        # j -> k \n",
    "        \n",
    "        i_scores = []\n",
    "        \n",
    "        for k in xrange(len(NER_tags)):\n",
    "            \n",
    "            score_k = []\n",
    "            \n",
    "            for j in xrange(len(NER_tags)):\n",
    "                \n",
    "                score_k.append(l1*scores[i,:,k] + l2*Transition_matrix[j,k] + i_1_scores[:,j])\n",
    "                \n",
    "            score_k = tf.convert_to_tensor(score_k)\n",
    "            \n",
    "            score_k = tf.transpose(score_k)\n",
    "            \n",
    "            max_k = tf.reduce_max(score_k,1)\n",
    "            \n",
    "            score_k = score_k-tf.reshape(max_k,[-1,1])\n",
    "            score_k = tf.exp(score_k)\n",
    "            \n",
    "            score_k = tf.reduce_sum(score_k,1)\n",
    "            \n",
    "            score_k = max_k + tf.log(score_k)\n",
    "                \n",
    "            i_scores.append(score_k)\n",
    "        \n",
    "        i_1_scores = tf.convert_to_tensor(i_scores)\n",
    "        \n",
    "        # now i_1_scores = NER_tags_len x batch_size\n",
    "        \n",
    "        i_1_scores = tf.transpose(i_1_scores)\n",
    "        \n",
    "        return i+1,i_1_scores\n",
    "    \n",
    "    _,seq_len_scores = tf.while_loop(cond,body,[i,i_1_scores])\n",
    "    \n",
    "    # now seq_lens = batch_size x NER_tags_len\n",
    "    max_scores = tf.reduce_max(seq_len_scores,1)\n",
    "    seq_len_scores = seq_len_scores - tf.reshape(max_scores,[-1,1])\n",
    "    seq_len_scores = tf.exp(seq_len_scores)\n",
    "    Z = tf.reduce_sum(seq_len_scores,1)\n",
    "    logZ = max_scores + tf.log(Z)\n",
    "        \n",
    "    y_score = tf.zeros([tf_batch_size])\n",
    "\n",
    "        \n",
    "    y_score = tf.cond(tf.equal(traintestval,True),\n",
    "                      lambda:fn1(tf_batch_size,seq_len,y_score,scores),\n",
    "                      lambda:fn2(y_score))\n",
    "\n",
    "\n",
    "    return GRU_scores,logZ,y_score\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_scores,logZ,y_score = model(traintestval)\n",
    "\n",
    "log_PyX = y_score - logZ\n",
    "\n",
    "# l2 regularization\n",
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "regularization = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "\n",
    "# Define loss and optimizer\n",
    "\n",
    "#GRU_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=GRU_scores, labels=tf_tags))+regularization\n",
    "cost = -tf.reduce_mean(log_PyX) + regularization\n",
    "\n",
    "\n",
    "#optimizer_GRU = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(GRU_cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function:\n",
    "\n",
    "This function computes the most probable sequence given the GRU scores and the transition matrix values. \n",
    "While iterating through each position the code, keeps the maximum scoring sequence ending with tag k at i_scores[k].\n",
    "At the end, we will have the maximum scoring sequences ending with tag K (where k=0:no.of_classes-1).\n",
    "We can then take the maximum out of it.\n",
    "The paths of the sequence ending at k, are stored in Paths[:,k] which is updated at every step. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(T,GRU_scores):\n",
    "    \n",
    "    batch_size = GRU_scores.shape[0]\n",
    "    seq_len = GRU_scores.shape[1]\n",
    "    \n",
    "    GRU_scores = np.transpose(GRU_scores,[1,0,2])\n",
    "    \n",
    "    GRU_scores = GRU_scores*scale_down\n",
    "    \n",
    "    i_1_scores = GRU_scores[0]\n",
    "    \n",
    "    paths = np.zeros((batch_size,len(NER_tags),seq_len),np.int32)\n",
    "    \n",
    "    for b in xrange(batch_size):\n",
    "        \n",
    "        for i in xrange(len(NER_tags)):\n",
    "            \n",
    "            paths[b,i,0] = i\n",
    "    \n",
    "    for i in xrange(len(GRU_scores)):\n",
    "        \n",
    "        new_paths = np.zeros((batch_size,len(NER_tags),seq_len),np.int32)\n",
    "        i_1_scores_temp = np.copy(i_1_scores)\n",
    "        \n",
    "        for k in xrange(len(NER_tags)):\n",
    "            \n",
    "            score_k = []\n",
    "            \n",
    "            for j in xrange(len(NER_tags)):\n",
    "                \n",
    "                score_ijk = GRU_scores[i,:,k]+T[j,k]+i_1_scores[:,j]\n",
    "                score_k.append(score_ijk)\n",
    "                \n",
    "            score_k = np.asarray(score_k,np.float32)\n",
    "            # score_k size = NER_tags x batch_size\n",
    "            \n",
    "            score_k = np.transpose(score_k)\n",
    "            \n",
    "            best_j = np.argmax(score_k,1)\n",
    "            \n",
    "            k_max_scores = np.amax(score_k,1)\n",
    "            \n",
    "            for b in xrange(batch_size):\n",
    "                \n",
    "                new_paths[b,k] = paths[b,best_j[b]]\n",
    "                new_paths[b,k,i] = k\n",
    "                i_1_scores_temp[b,k] = k_max_scores[b]\n",
    "            \n",
    "        paths = np.copy(new_paths)\n",
    "        i_1_scores = np.copy(i_1_scores_temp)\n",
    "    \n",
    "    optimal_seq = np.zeros((batch_size,seq_len),np.int32)\n",
    "    \n",
    "    # scores_i_1 = NER_tags x batch_size\n",
    "    \n",
    "    for b in xrange(batch_size):\n",
    "        \n",
    "        best_seq_end_tag = np.argmax(i_1_scores[b,:])\n",
    "        optimal_seq[b] = paths[b,best_seq_end_tag]\n",
    "        \n",
    "    return optimal_seq   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_plain_acc(predicted_seq,tags):\n",
    "    \n",
    "    acc_tensor = np.equal(predicted_seq,tags)\n",
    "    acc_tensor.astype(np.float32)\n",
    "    acc = np.mean(acc_tensor)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 100.976, Accuracy= 0.467\n",
      "Iter 20, Loss= 25.402, Accuracy= 91.300\n",
      "Iter 40, Loss= 13.638, Accuracy= 95.100\n",
      "Iter 60, Loss= 9.456, Accuracy= 95.067\n",
      "\n",
      "Epoch 1, Validation Loss= 6.851, validation Accuracy= 95.453%\n",
      "Epoch 1, Average Training Loss= 19.559, Average Training Accuracy= 91.532%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 6.885, Accuracy= 95.567\n",
      "Iter 20, Loss= 5.857, Accuracy= 95.567\n",
      "Iter 40, Loss= 4.720, Accuracy= 96.300\n",
      "Iter 60, Loss= 4.283, Accuracy= 96.067\n",
      "\n",
      "Epoch 2, Validation Loss= 3.736, validation Accuracy= 96.440%\n",
      "Epoch 2, Average Training Loss= 4.955, Average Training Accuracy= 96.043%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 4.001, Accuracy= 95.800\n",
      "Iter 20, Loss= 3.207, Accuracy= 96.367\n",
      "Iter 40, Loss= 2.763, Accuracy= 97.000\n",
      "Iter 60, Loss= 2.623, Accuracy= 96.767\n",
      "\n",
      "Epoch 3, Validation Loss= 2.777, validation Accuracy= 96.953%\n",
      "Epoch 3, Average Training Loss= 2.967, Average Training Accuracy= 96.918%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 2.281, Accuracy= 97.333\n",
      "Iter 20, Loss= 2.133, Accuracy= 97.367\n",
      "Iter 40, Loss= 2.358, Accuracy= 97.200\n",
      "Iter 60, Loss= 2.209, Accuracy= 97.300\n",
      "\n",
      "Epoch 4, Validation Loss= 2.455, validation Accuracy= 96.983%\n",
      "Epoch 4, Average Training Loss= 2.222, Average Training Accuracy= 97.352%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 2.204, Accuracy= 97.133\n",
      "Iter 20, Loss= 1.731, Accuracy= 97.900\n",
      "Iter 40, Loss= 1.544, Accuracy= 97.700\n",
      "Iter 60, Loss= 1.542, Accuracy= 97.967\n",
      "\n",
      "Epoch 5, Validation Loss= 2.458, validation Accuracy= 96.890%\n",
      "Epoch 5, Average Training Loss= 1.827, Average Training Accuracy= 97.704%\n",
      "\n",
      "Iter 0, Loss= 1.806, Accuracy= 98.333\n",
      "Iter 20, Loss= 1.446, Accuracy= 98.033\n",
      "Iter 40, Loss= 1.365, Accuracy= 98.200\n",
      "Iter 60, Loss= 1.732, Accuracy= 97.600\n",
      "\n",
      "Epoch 6, Validation Loss= 2.434, validation Accuracy= 96.907%\n",
      "Epoch 6, Average Training Loss= 1.581, Average Training Accuracy= 97.930%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.211, Accuracy= 98.200\n",
      "Iter 20, Loss= 1.935, Accuracy= 97.433\n",
      "Iter 40, Loss= 1.550, Accuracy= 98.100\n",
      "Iter 60, Loss= 1.006, Accuracy= 98.133\n",
      "\n",
      "Epoch 7, Validation Loss= 2.571, validation Accuracy= 96.667%\n",
      "Epoch 7, Average Training Loss= 1.362, Average Training Accuracy= 98.166%\n",
      "\n",
      "Iter 0, Loss= 1.163, Accuracy= 98.567\n",
      "Iter 20, Loss= 1.033, Accuracy= 98.433\n",
      "Iter 40, Loss= 1.142, Accuracy= 98.067\n",
      "Iter 60, Loss= 1.414, Accuracy= 97.900\n",
      "\n",
      "Epoch 8, Validation Loss= 2.526, validation Accuracy= 96.827%\n",
      "Epoch 8, Average Training Loss= 1.203, Average Training Accuracy= 98.373%\n",
      "\n",
      "Iter 0, Loss= 0.858, Accuracy= 98.733\n",
      "Iter 20, Loss= 0.930, Accuracy= 98.467\n",
      "Iter 40, Loss= 1.033, Accuracy= 98.500\n",
      "Iter 60, Loss= 1.000, Accuracy= 98.000\n",
      "\n",
      "Epoch 9, Validation Loss= 2.613, validation Accuracy= 96.883%\n",
      "Epoch 9, Average Training Loss= 1.042, Average Training Accuracy= 98.503%\n",
      "\n",
      "Iter 0, Loss= 1.180, Accuracy= 98.733\n",
      "Iter 20, Loss= 0.876, Accuracy= 98.833\n",
      "Iter 40, Loss= 0.955, Accuracy= 98.567\n",
      "Iter 60, Loss= 0.924, Accuracy= 98.567\n",
      "\n",
      "Epoch 10, Validation Loss= 2.695, validation Accuracy= 96.943%\n",
      "Epoch 10, Average Training Loss= 0.921, Average Training Accuracy= 98.662%\n",
      "\n",
      "Iter 0, Loss= 0.652, Accuracy= 99.200\n",
      "Iter 20, Loss= 0.918, Accuracy= 98.500\n",
      "Iter 40, Loss= 0.815, Accuracy= 98.467\n",
      "Iter 60, Loss= 0.722, Accuracy= 98.933\n",
      "\n",
      "Epoch 11, Validation Loss= 2.772, validation Accuracy= 96.647%\n",
      "Epoch 11, Average Training Loss= 0.811, Average Training Accuracy= 98.769%\n",
      "\n",
      "Iter 0, Loss= 0.684, Accuracy= 98.867\n",
      "Iter 20, Loss= 0.616, Accuracy= 99.167\n",
      "Iter 40, Loss= 0.796, Accuracy= 98.900\n",
      "Iter 60, Loss= 1.050, Accuracy= 98.300\n",
      "\n",
      "Epoch 12, Validation Loss= 2.865, validation Accuracy= 96.767%\n",
      "Epoch 12, Average Training Loss= 0.738, Average Training Accuracy= 98.875%\n",
      "\n",
      "Iter 0, Loss= 0.542, Accuracy= 99.133\n",
      "Iter 20, Loss= 0.648, Accuracy= 98.933\n",
      "Iter 40, Loss= 0.589, Accuracy= 99.100\n",
      "Iter 60, Loss= 0.470, Accuracy= 99.000\n",
      "\n",
      "Epoch 13, Validation Loss= 2.988, validation Accuracy= 96.503%\n",
      "Epoch 13, Average Training Loss= 0.692, Average Training Accuracy= 98.867%\n",
      "\n",
      "Iter 0, Loss= 0.629, Accuracy= 98.767\n",
      "Iter 20, Loss= 0.613, Accuracy= 99.033\n",
      "Iter 40, Loss= 0.643, Accuracy= 98.767\n",
      "Iter 60, Loss= 1.052, Accuracy= 98.533\n",
      "\n",
      "Epoch 14, Validation Loss= 2.998, validation Accuracy= 96.477%\n",
      "Epoch 14, Average Training Loss= 0.643, Average Training Accuracy= 98.930%\n",
      "\n",
      "Iter 0, Loss= 0.682, Accuracy= 99.033\n",
      "Iter 20, Loss= 0.802, Accuracy= 98.867\n",
      "Iter 40, Loss= 0.566, Accuracy= 99.200\n",
      "Iter 60, Loss= 0.556, Accuracy= 98.900\n",
      "\n",
      "Epoch 15, Validation Loss= 3.026, validation Accuracy= 96.713%\n",
      "Epoch 15, Average Training Loss= 0.593, Average Training Accuracy= 98.947%\n",
      "\n",
      "Iter 0, Loss= 0.422, Accuracy= 99.267\n",
      "Iter 20, Loss= 0.573, Accuracy= 98.767\n",
      "Iter 40, Loss= 0.593, Accuracy= 99.000\n",
      "Iter 60, Loss= 0.555, Accuracy= 98.700\n",
      "\n",
      "Epoch 16, Validation Loss= 3.152, validation Accuracy= 96.647%\n",
      "Epoch 16, Average Training Loss= 0.580, Average Training Accuracy= 98.900%\n",
      "\n",
      "Iter 0, Loss= 0.355, Accuracy= 99.400\n",
      "Iter 20, Loss= 0.557, Accuracy= 98.967\n",
      "Iter 40, Loss= 0.464, Accuracy= 98.700\n",
      "Iter 60, Loss= 0.625, Accuracy= 98.667\n",
      "\n",
      "Epoch 17, Validation Loss= 3.187, validation Accuracy= 96.567%\n",
      "Epoch 17, Average Training Loss= 0.527, Average Training Accuracy= 98.987%\n",
      "\n",
      "Iter 0, Loss= 0.379, Accuracy= 98.933\n",
      "Iter 20, Loss= 0.456, Accuracy= 98.900\n",
      "Iter 40, Loss= 0.456, Accuracy= 99.067\n",
      "Iter 60, Loss= 0.398, Accuracy= 99.233\n",
      "\n",
      "Epoch 18, Validation Loss= 3.244, validation Accuracy= 96.437%\n",
      "Epoch 18, Average Training Loss= 0.502, Average Training Accuracy= 99.005%\n",
      "\n",
      "Iter 0, Loss= 0.535, Accuracy= 98.867\n",
      "Iter 20, Loss= 0.409, Accuracy= 99.433\n",
      "Iter 40, Loss= 0.633, Accuracy= 98.667\n",
      "Iter 60, Loss= 0.551, Accuracy= 98.933\n",
      "\n",
      "Epoch 19, Validation Loss= 3.338, validation Accuracy= 96.370%\n",
      "Epoch 19, Average Training Loss= 0.518, Average Training Accuracy= 98.955%\n",
      "\n",
      "Iter 0, Loss= 0.463, Accuracy= 98.867\n",
      "Iter 20, Loss= 0.323, Accuracy= 99.467\n",
      "Iter 40, Loss= 0.370, Accuracy= 99.133\n",
      "Iter 60, Loss= 0.497, Accuracy= 98.933\n",
      "\n",
      "Epoch 20, Validation Loss= 3.347, validation Accuracy= 96.513%\n",
      "Epoch 20, Average Training Loss= 0.499, Average Training Accuracy= 98.924%\n",
      "\n",
      "Iter 0, Loss= 0.312, Accuracy= 99.267\n",
      "Iter 20, Loss= 0.342, Accuracy= 99.333\n",
      "Iter 40, Loss= 0.308, Accuracy= 99.200\n",
      "Iter 60, Loss= 0.376, Accuracy= 98.867\n",
      "\n",
      "Epoch 21, Validation Loss= 3.414, validation Accuracy= 96.500%\n",
      "Epoch 21, Average Training Loss= 0.443, Average Training Accuracy= 99.000%\n",
      "\n",
      "Iter 0, Loss= 0.295, Accuracy= 99.000\n",
      "Iter 20, Loss= 0.343, Accuracy= 99.300\n",
      "Iter 40, Loss= 0.364, Accuracy= 99.200\n",
      "Iter 60, Loss= 0.541, Accuracy= 98.700\n",
      "\n",
      "Epoch 22, Validation Loss= 3.475, validation Accuracy= 96.557%\n",
      "Epoch 22, Average Training Loss= 0.437, Average Training Accuracy= 99.010%\n",
      "\n",
      "Iter 0, Loss= 0.477, Accuracy= 99.100\n",
      "Iter 20, Loss= 0.509, Accuracy= 98.733\n",
      "Iter 40, Loss= 0.311, Accuracy= 99.233\n",
      "Iter 60, Loss= 0.460, Accuracy= 99.167\n",
      "\n",
      "Epoch 23, Validation Loss= 3.390, validation Accuracy= 96.403%\n",
      "Epoch 23, Average Training Loss= 0.432, Average Training Accuracy= 99.002%\n",
      "\n",
      "Iter 0, Loss= 0.295, Accuracy= 99.167\n",
      "Iter 20, Loss= 0.573, Accuracy= 98.567\n",
      "Iter 40, Loss= 0.372, Accuracy= 98.867\n",
      "Iter 60, Loss= 0.447, Accuracy= 99.333\n",
      "\n",
      "Epoch 24, Validation Loss= 3.422, validation Accuracy= 96.333%\n",
      "Epoch 24, Average Training Loss= 0.438, Average Training Accuracy= 98.968%\n",
      "\n",
      "Iter 0, Loss= 0.357, Accuracy= 99.167\n",
      "Iter 20, Loss= 0.330, Accuracy= 99.200\n",
      "Iter 40, Loss= 0.322, Accuracy= 99.033\n",
      "Iter 60, Loss= 0.454, Accuracy= 99.100\n",
      "\n",
      "Epoch 25, Validation Loss= 3.452, validation Accuracy= 96.360%\n",
      "Epoch 25, Average Training Loss= 0.441, Average Training Accuracy= 98.934%\n",
      "\n",
      "Iter 0, Loss= 0.300, Accuracy= 98.767\n",
      "Iter 20, Loss= 0.312, Accuracy= 99.133\n",
      "Iter 40, Loss= 0.475, Accuracy= 98.633\n",
      "Iter 60, Loss= 0.432, Accuracy= 98.933\n",
      "\n",
      "Epoch 26, Validation Loss= 3.425, validation Accuracy= 96.280%\n",
      "Epoch 26, Average Training Loss= 0.465, Average Training Accuracy= 98.833%\n",
      "\n",
      "Iter 0, Loss= 0.286, Accuracy= 99.200\n",
      "Iter 20, Loss= 0.460, Accuracy= 99.100\n",
      "Iter 40, Loss= 0.424, Accuracy= 99.100\n",
      "Iter 60, Loss= 0.574, Accuracy= 98.867\n",
      "\n",
      "Epoch 27, Validation Loss= 3.443, validation Accuracy= 96.493%\n",
      "Epoch 27, Average Training Loss= 0.466, Average Training Accuracy= 98.819%\n",
      "\n",
      "Early Stopping since best validation loss not decreasing for 20 epochs.\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "Best Validation Loss: 2.434\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_loss=2**30\n",
    "    prev_val_loss=2**30\n",
    "    patience = 20\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "    epochs = 200\n",
    "            \n",
    "    batch_size = 100\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_sentences,batches_tags = create_batches(train_sentences,train_tags,batch_size)\n",
    "\n",
    "        for i in xrange(len(batches_sentences)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,GRU_scores_out,T,npZ,npy_score = sess.run([optimizer,cost,GRU_scores,\n",
    "                                                          Transition_matrix,logZ,log_PyX],\n",
    "                                      feed_dict={tf_sentences: batches_sentences[i], \n",
    "                                                  tf_tags: batches_tags[i],\n",
    "                                                  traintestval: True})\n",
    "\n",
    "            total_loss += loss\n",
    "            \n",
    "            predicted_seq = predict(T,GRU_scores_out)\n",
    "            acc = measure_plain_acc(predicted_seq,batches_tags[i])\n",
    "            \n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_sentences) \n",
    "        avg_acc = total_acc/len(batches_sentences)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 100\n",
    "        batches_sentences,batches_tags = create_batches(val_sentences,val_tags,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_sentences)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            val_loss,val_GRU_scores,val_T = sess.run([cost,GRU_scores,Transition_matrix],\n",
    "                                       feed_dict={tf_sentences: batches_sentences[i], \n",
    "                                                  tf_tags: batches_tags[i],\n",
    "                                                  traintestval: True})\n",
    "            total_val_loss += val_loss\n",
    "            \n",
    "            predicted_seq = predict(val_T,val_GRU_scores)\n",
    "            val_acc = measure_plain_acc(predicted_seq,batches_tags[i])\n",
    "            \n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_sentences) \n",
    "        avg_val_acc = total_val_acc/len(batches_sentences) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "        \n",
    "        impatience += 1\n",
    "        \n",
    "        if avg_val_loss <= best_val_loss: \n",
    "            impatience = 0\n",
    "            best_val_loss = avg_val_loss\n",
    "            saver.save(sess, 'CRF_Model_Backup/model.ckpt') \n",
    "            print \"Checkpoint created!\"  \n",
    "        \n",
    "        if impatience > patience:\n",
    "            print \"\\nEarly Stopping since best validation loss not decreasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Loss: %.3f\"%((best_val_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing.....\n",
    "\n",
    "Note: This is just the normal accuracy. I haven't calculated the F1 score which will probably be much lower.\n",
    "The dataset is pretty skewed. First most words are not named entities and have the tag 'O' - that's natural, but also makes the data biased.\n",
    "ON TOP OF THAT, I used PADDING WITHOUT BUCKETING. \n",
    "One can achieve a fairly high accuracy just by predicting all words as 'O'. F1 accuracy is needed to make a fair estimate.\n",
    "\n",
    "I will implement it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for the model...\n",
      "INFO:tensorflow:Restoring parameters from CRF_Model_Backup/model.ckpt\n",
      "\n",
      "RESTORATION COMPLETE\n",
      "\n",
      "Testing Model Performance...\n",
      "\n",
      "Test Loss= 2.470, Test Accuracy= 96.777%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Begin session\n",
    "    \n",
    "    print 'Loading pre-trained weights for the model...'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'CRF_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    print '\\nRESTORATION COMPLETE\\n'\n",
    "    \n",
    "    print 'Testing Model Performance...'\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    \n",
    "    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n",
    "    batches_sentences,batches_tags = create_batches(test_sentences,test_tags,test_batch_size)\n",
    "        \n",
    "    for i in xrange(len(batches_sentences)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            test_loss,T,GRU_out = sess.run([cost,Transition_matrix,GRU_scores],\n",
    "                                       feed_dict={tf_sentences: batches_sentences[i], \n",
    "                                                  tf_tags: batches_tags[i],\n",
    "                                                  traintestval: True})\n",
    "            total_test_loss += test_loss\n",
    "            predicted_seq = predict(T,GRU_out)\n",
    "            test_acc = measure_plain_acc(predicted_seq,batches_tags[i])\n",
    "            total_test_acc += test_acc\n",
    "                      \n",
    "            \n",
    "    avg_test_loss = total_test_loss/len(batches_sentences) \n",
    "    avg_test_acc = total_test_acc/len(batches_sentences) \n",
    "\n",
    "\n",
    "    print \"\\nTest Loss= \" + \\\n",
    "          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n",
    "          \"{:.3f}%\".format(avg_test_acc*100)+\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on a random test sample\n",
    "\n",
    "The model seems to be working at least.\n",
    "This cell can be run more than one times, and the prediction of different test sequences can be compared with the actual tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE ABOUT TO BE FED:\n",
      "\n",
      "['pakistani', 'military', 'authorities', 'say', 'they', 'have', 'arrested', 'a', 'key', 'taliban', 'commander', 'who', 'is', 'accused', 'of', 'slaughtering', 'military', 'personnel', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from CRF_Model_Backup/model.ckpt\n",
      "\n",
      "RESULT:\n",
      "\n",
      "pakistani - Tag: B-gpe Prediction: B-gpe\n",
      "military - Tag: O Prediction: O\n",
      "authorities - Tag: O Prediction: O\n",
      "say - Tag: O Prediction: O\n",
      "they - Tag: O Prediction: O\n",
      "have - Tag: O Prediction: O\n",
      "arrested - Tag: O Prediction: O\n",
      "a - Tag: O Prediction: O\n",
      "key - Tag: O Prediction: O\n",
      "taliban - Tag: B-org Prediction: B-org\n",
      "commander - Tag: O Prediction: O\n",
      "who - Tag: O Prediction: O\n",
      "is - Tag: O Prediction: O\n",
      "accused - Tag: O Prediction: O\n",
      "of - Tag: O Prediction: O\n",
      "slaughtering - Tag: O Prediction: O\n",
      "military - Tag: O Prediction: O\n",
      "personnel - Tag: O Prediction: O\n",
      ". - Tag: O Prediction: O\n",
      "\n",
      "\n",
      "ACCURACY: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand = random.randint(0,len(test_sentences))\n",
    "\n",
    "print \"SENTENCE ABOUT TO BE FED:\\n\"\n",
    "print map(vec2word,test_sentences[rand])\n",
    "    \n",
    "print \"\\n\"\n",
    "\n",
    "with tf.Session() as sess: # Begin session\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'CRF_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    scores,T = sess.run([GRU_scores,Transition_matrix],\n",
    "                          feed_dict={tf_sentences: [test_sentences[rand]],\n",
    "                          tf_tags: [test_tags[rand]],\n",
    "                          traintestval: True})\n",
    "    prediction = predict(T,scores)\n",
    "    acc = measure_plain_acc(prediction,[test_tags[rand]])\n",
    "\n",
    "print \"\\nRESULT:\\n\"\n",
    "\n",
    "for i in xrange(len(prediction[0])):\n",
    "    ##removing Prediction of pads\n",
    "    if vocab.index(vec2word(test_sentences[rand,i])) == vocab.index('<PAD>'):\n",
    "        break\n",
    "    print vec2word(test_sentences[rand,i]),\n",
    "    print \"- Tag: \"+NER_tags[test_tags[rand,i]],\n",
    "    print \"Prediction: \"+NER_tags[prediction[0,i]]\n",
    "    \n",
    "    \n",
    "print \"\\n\\nACCURACY: \"+str(acc*100)+\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "A very lame implementation of a cell that predicts user given sentences.\n",
    "One has to enter a sentence in the code in a tokenized lower case format.\n",
    "I may polish this part later, and make it more user friendly, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE ABOUT TO BE FED: \n",
      "['the', 'british', 'is', 'planning', 'to', 'attack', 'india', 'at', 'eleven']\n",
      "\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from CRF_Model_Backup/model.ckpt\n",
      "\n",
      "RESULT:\n",
      "\n",
      "the - O\n",
      "british - B-gpe\n",
      "is - O\n",
      "planning - O\n",
      "to - O\n",
      "attack - O\n",
      "india - B-geo\n",
      "at - O\n",
      "eleven - O\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rand = random.randint(0,len(sentences))\n",
    "\n",
    "sentence = ['the','british','is','planning','to','attack','india','at','eleven']\n",
    "# ENTER YOUR OWN SENTENCE HERE IN THIS FORMAT; LOWER CASE PLEASE.\n",
    "\n",
    "print \"SENTENCE ABOUT TO BE FED: \"\n",
    "print sentence\n",
    "    \n",
    "print \"\\n\"\n",
    "\n",
    "sentence_vec = map(word2vec,sentence)\n",
    "\n",
    "with tf.Session() as sess: # Begin session\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'CRF_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    T,scores = sess.run([Transition_matrix,GRU_scores],\n",
    "                          feed_dict={tf_sentences: [sentence_vec],\n",
    "                                     traintestval: False})\n",
    "    prediction = predict(T,scores)\n",
    "\n",
    "print \"\\nRESULT:\\n\"\n",
    "for i in xrange(len(prediction[0])):\n",
    "    print sentence[i],\n",
    "    print \"- \"+NER_tags[prediction[0,i]]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
