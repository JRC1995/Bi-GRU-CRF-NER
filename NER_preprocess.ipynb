{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 41417\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "    \n",
    "sentences=[]\n",
    "tags=[]\n",
    "NER_tags = []\n",
    "max_len = 30\n",
    "\n",
    "with open('ner.csv') as csvfile:\n",
    "    \n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \n",
    "    sentence=[]\n",
    "    tag=[]\n",
    "    i=0\n",
    "    \n",
    "    for row in reader:\n",
    "        if row['Sentence #']!='' and i!=0:\n",
    "            \n",
    "            if len(sentence) <= max_len:\n",
    "                sentences.append(sentence)\n",
    "                tags.append(tag)\n",
    "            sentence=[]\n",
    "            tag=[]\n",
    "            \n",
    "        sentence.append(row['Word'].lower())\n",
    "        \n",
    "        temp = row['Tag']\n",
    "\n",
    "        if temp not in NER_tags:\n",
    "            NER_tags.append(temp)\n",
    "        \n",
    "        tag.append(NER_tags.index(temp))\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "    print \"No. of samples: \"+str(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim', 'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve', 'I-eve', 'I-nat']\n"
     ]
    }
   ],
   "source": [
    "print NER_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enough = 10000\n",
    "\n",
    "sentences = sentences[0:enough]\n",
    "tags = tags[0:enough]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    vectorized_sentences.append(map(word2vec,sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = np.zeros((len(sentences),max_len,word_vec_dim),np.float32)\n",
    "padded_tags = np.zeros((len(tags),max_len),np.int32)\n",
    "\n",
    "pad_word = np.zeros((word_vec_dim),np.float32)\n",
    "pad_tag = NER_tags.index('O')\n",
    "\n",
    "for i in xrange(len(sentences)):\n",
    "    \n",
    "    for j in xrange(max_len):\n",
    "        \n",
    "        if j >= len(sentences[i]):\n",
    "            padded_sentences[i,j] = pad_word\n",
    "            padded_tags[i,j] = pad_tag\n",
    "        else:\n",
    "            padded_sentences[i,j] = vectorized_sentences[i][j]\n",
    "            padded_tags[i,j] = tags[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving processed data in another file.\n",
    "\n",
    "import pickle\n",
    "\n",
    "PICK = [NER_tags,padded_sentences,padded_tags]\n",
    "\n",
    "with open('NERPICKLE_10000', 'wb') as fp:\n",
    "    pickle.dump(PICK, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
